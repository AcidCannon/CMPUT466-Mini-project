{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZtHHATklPMY6e7G5b5Y83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AcidCannon/CMPUT466-Mini-project/blob/master/task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzybJHQBKjKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QpuRQT7LCi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {}\n",
        "config['training_size'] = 50000\n",
        "config['training_batch_size'] = 64\n",
        "config['training_shuffle'] = True\n",
        "config['device'] = 'gpu'\n",
        "config['generator_learning_rate'] = 0.001\n",
        "config['generator_weight_decay'] = 0.001\n",
        "config['discriminator_learning_rate'] = 0.001 \n",
        "config['discriminator_weight_decay'] = 0.001\n",
        "config['algorithm'] = 'Generative Adversarial Network'\n",
        "config['number_of_epochs'] = 1000\n",
        "config['discriminator_real_loss_coe'] = 1.0\n",
        "config['discriminator_fake_loss_coe'] = 1.0\n",
        "config['mode'] = 'fresh_start' # 'fresh_start' 'load_and_train' 'test' 'demo'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDrP0S7hXE06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if config['mode'] == 'demo':\n",
        "  # load last time trained model and data\n",
        "  # FILL HERE WHEN DONE\n",
        "else:\n",
        "  # mount google drive to save checkpoints\n",
        "  from google.colab import drive\n",
        "  drive.mount('content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSf9MRLYLXXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(config):\n",
        "  MNIST_training_set = datasets.MNIST(root='data', train=True, download=True, transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5,), (0.5,))\n",
        "                  ]))\n",
        "  \n",
        "  MNIST_training_set = torch.utils.data.Subset(MNIST_training_set, range(0, config['training_size']))\n",
        "\n",
        "  training_dataloader = torch.utils.data.DataLoader(dataset=MNIST_training_set, batch_size=config['training_batch_size'], shuffle=config['training_shuffle'])\n",
        "\n",
        "  return training_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uihkhkONOex3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.fc1 = nn.Linear(100, 32)\n",
        "    self.fc2 = nn.Linear(32, 64)\n",
        "    self.fc3 = nn.Linear(64, 128)\n",
        "    self.fc4 = nn.Linear(128, 784)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2, True)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2, True)\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2, True)\n",
        "    return torch.tanh(self.fc4(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxXOYlUePwsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 32)\n",
        "    self.fc4 = nn.Linear(32, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # using dropout to prevent overfitting\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2, True)\n",
        "    x = F.dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2, True)\n",
        "    x = F.dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2, True)\n",
        "    x = F.dropout(x)\n",
        "    return torch.sigmoid(self.fc4(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utIUbQscSIDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_train(config, data, discriminator_optimizer, discriminator, generator, loss_function):\n",
        "  # set gradient to be zero\n",
        "  discriminator_optimizer.zero_grad()\n",
        "  \n",
        "  real_data = (data.view(-1, 784)).to(config['device'])\n",
        "  real_target = (torch.ones(data.shape[0], 1)).to(config['device'])\n",
        "\n",
        "  # forward propagation for real\n",
        "  output = discriminator(real_data)\n",
        "  # compute loss for real\n",
        "  discriminator_real_loss = loss_function(output, real_target)\n",
        "\n",
        "  fake_data = generator(torch.randn(data.shape[0], 100).to(config['device']))\n",
        "  fake_target = (torch.zeros(data.shape[0], 1)).to(config['device'])\n",
        "\n",
        "  # forward propagation for fake\n",
        "  output = discriminator(fake_data)\n",
        "  # compute loss for fake\n",
        "  discriminator_fake_loss = loss_function(output, fake_target)\n",
        "\n",
        "  # compute total loss as a combination of real loss and fake loss\n",
        "  discriminator_total_loss = config['discriminator_real_loss_coe']*discriminator_real_loss + config['discriminator_fake_loss_coe']*discriminator_fake_loss\n",
        "  \n",
        "  # backward propagation for total loss\n",
        "  discriminator_total_loss.backward()\n",
        "  \n",
        "  # weight updation\n",
        "  discriminator_optimizer.step()\n",
        "  \n",
        "  return discriminator_total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0jpk0GScQIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_train(config, data, generator_optimizer, discriminator, generator, loss_function):\n",
        "  # set gradient to be zero\n",
        "  generator_optimizer.zero_grad()\n",
        "\n",
        "  # let generator generates fake image and see if discriminator can discriminate it\n",
        "  # first forward propagate for generator\n",
        "  fake_data = generator(torch.randn(data.shape[0], 100).to(config['device']))\n",
        "  real_target = (torch.ones(data.shape[0], 1)).to(config['device'])\n",
        "\n",
        "  # then forward propagate for discriminator\n",
        "  output = discriminator(fake_data)\n",
        "\n",
        "  # compute loss for generator\n",
        "  generator_total_loss = loss_function(output, real_target)\n",
        "\n",
        "  # backward propagation for total loss\n",
        "  generator_total_loss.backward()\n",
        "\n",
        "  # weight updation\n",
        "  generator_optimizer.step()\n",
        "\n",
        "  return generator_total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyy-4fgSdd1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(training_dataloader, config, device):\n",
        "  #### MAY MODIFY LATER ####\n",
        "  loss_function = F.binary_cross_entropy\n",
        "  if config['mode'] == 'load_and_train':\n",
        "    discriminator = config['discriminator_loaded']\n",
        "    generator = config['generator_loaded']\n",
        "  elif config['mode'] == 'fresh_start':\n",
        "    discriminator = Discriminator().to(device)\n",
        "    generator = Generator().to(device)\n",
        "  discriminator.train()\n",
        "  generator.train()\n",
        "  discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=config['discriminator_learning_rate'], weight_decay = config['discriminator_weight_decay'])\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr=config['generator_learning_rate'], weight_decay=config['generator_weight_decay'])\n",
        "  #### MAY MODIFY END ####\n",
        "  for epoch in range(1, config['number_of_epochs']+1):\n",
        "    discriminator_loss = 0\n",
        "    discriminator_count = 0\n",
        "    generator_loss = 0\n",
        "    generator_count = 0\n",
        "    for idx, (data, target) in enumerate(training_dataloader):\n",
        "      discriminator_loss += discriminator_train(config, data, discriminator_optimizer, discriminator, generator, loss_function)\n",
        "      discriminator_count += 1\n",
        "      generator_loss += generator_train(config, data, generator_optimizer, discriminator, generator, loss_function)\n",
        "      generator_count += 1\n",
        "    print(\"Epoch: {}.\\tGenerator Avg.loss: {}.\\tDiscriminator Avg.loss: {}.\".format(epoch, generator_loss/generator_count, discriminator_loss/discriminator_count))\n",
        "    if config['mode'] == 'fresh_start' or config['mode'] == 'load_and_train':\n",
        "      torch.save(generator.state_dict(), '/content/gdrive/My Drive/checkpoints/g.ckpt.{}.pth'.format(epoch))\n",
        "      torch.save(discriminator.state_dict(), '/content/gdrive/My Drive/checkpoints/d.ckpt.{}.pth'.format(epoch))\n",
        "      print('Checkpoint saved.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzYM1WTGn2Za",
        "colab_type": "code",
        "outputId": "f74a582d-c045-48df-8cd0-c3fc8c9c906e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def run(config):\n",
        "  print('Training set size: {}x{}.'.format(config['training_size'], '1x32x32'))\n",
        "  print('Using algorithm: {}.'.format(config['algorithm']))\n",
        "  print('Running in {} mode.'.format(config['mode']))\n",
        "\n",
        "  if config['device'] != 'cpu' and torch.cuda.is_available():\n",
        "    config['device'] = torch.device('cuda')\n",
        "    print('Using GPU: {}.'.format(torch.cuda.get_device_name(0)))\n",
        "  else:\n",
        "    config['device'] = torch.device('cpu')\n",
        "    print('Using CPU.')\n",
        "\n",
        "  print(\"Running...\")\n",
        "\n",
        "  if config['mode'] == 'test':\n",
        "    generator = Generator().to(config['device'])\n",
        "    discriminator = Discriminator().to(config['device'])\n",
        "\n",
        "    generator.load_state_dict(torch.load('/content/gdrive/My Drive/checkpoints/g.ckpt.pth'))\n",
        "    discriminator.load_state_dict(torch.load('/content/gdrive/My Drive/checkpoints/d.ckpt.pth'))\n",
        "\n",
        "    with torch.no_grad():\n",
        "      noize = torch.randn(10, 100).to(config['device'])\n",
        "      generated = generator(noize)\n",
        "\n",
        "      # prepare(reshape) noize to plot\n",
        "      # FILL ME\n",
        "      # prepare(reshape) generated image to plot\n",
        "      # FILL ME\n",
        "      # plot\n",
        "      # FILL ME\n",
        "  elif config['mode'] == 'fresh_start':  \n",
        "    train(load_data(config), config, config['device'])\n",
        "  elif config['mode'] == 'load_and_train':\n",
        "    generator = Generator().to(config['device'])\n",
        "    discriminator = Discriminator().to(config['device'])\n",
        "\n",
        "    generator.load_state_dict(torch.load('/content/gdrive/My Drive/checkpoints/g.ckpt.pth'))\n",
        "    discriminator.load_state_dict(torch.load('/content/gdrive/My Drive/checkpoints/d.ckpt.pth'))\n",
        "\n",
        "    config['generator_loaded'] = generator\n",
        "    config['discriminator_loaded'] = discriminator\n",
        "\n",
        "    train(load_data(config), config, config['device'])\n",
        "  elif config['mode'] == 'demo':\n",
        "    for epoch in range(0, 100+10, 10):\n",
        "      if epoch == 0:\n",
        "        # FILL ME\n",
        "        continue\n",
        "      generator = Generator().to(config['device'])\n",
        "      discriminator = Discriminator().to(config['device'])\n",
        "\n",
        "      generator.load_state_dict(torch.load('/content/gdrive/My Drive/checkpoints/g.ckpt.{}.pth'.format(epoch)))\n",
        "      discriminator.load_state_dict(torch.load('/content/gdrive/My Drive/checkpoints/d.ckpt.{}.pth'.format(epoch)))"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla P100-PCIE-16GB.\n",
            "Epoch: 1.\tGenerator Avg.loss: 1.267493486404419.\tDiscriminator Avg.loss: 1.0904977321624756.\n",
            "Epoch: 2.\tGenerator Avg.loss: 1.478269100189209.\tDiscriminator Avg.loss: 0.9976075291633606.\n",
            "Epoch: 3.\tGenerator Avg.loss: 1.2999438047409058.\tDiscriminator Avg.loss: 0.9958397746086121.\n",
            "Epoch: 4.\tGenerator Avg.loss: 1.298497200012207.\tDiscriminator Avg.loss: 0.9854564666748047.\n",
            "Epoch: 5.\tGenerator Avg.loss: 1.3774237632751465.\tDiscriminator Avg.loss: 0.9433188438415527.\n",
            "Epoch: 6.\tGenerator Avg.loss: 1.3769748210906982.\tDiscriminator Avg.loss: 0.9554319381713867.\n",
            "Epoch: 7.\tGenerator Avg.loss: 1.3774218559265137.\tDiscriminator Avg.loss: 0.9501277804374695.\n",
            "Epoch: 8.\tGenerator Avg.loss: 1.5706912279129028.\tDiscriminator Avg.loss: 0.8383976221084595.\n",
            "Epoch: 9.\tGenerator Avg.loss: 1.3721686601638794.\tDiscriminator Avg.loss: 0.9460525512695312.\n",
            "Epoch: 10.\tGenerator Avg.loss: 1.316267967224121.\tDiscriminator Avg.loss: 0.9900488257408142.\n",
            "Epoch: 11.\tGenerator Avg.loss: 1.3759726285934448.\tDiscriminator Avg.loss: 0.9481877088546753.\n",
            "Epoch: 12.\tGenerator Avg.loss: 1.2705931663513184.\tDiscriminator Avg.loss: 1.069777250289917.\n",
            "Epoch: 13.\tGenerator Avg.loss: 1.20767080783844.\tDiscriminator Avg.loss: 1.0570993423461914.\n",
            "Epoch: 14.\tGenerator Avg.loss: 1.3169666528701782.\tDiscriminator Avg.loss: 0.9693766832351685.\n",
            "Epoch: 15.\tGenerator Avg.loss: 1.1781233549118042.\tDiscriminator Avg.loss: 1.074939250946045.\n",
            "Epoch: 16.\tGenerator Avg.loss: 1.2607142925262451.\tDiscriminator Avg.loss: 1.0209455490112305.\n",
            "Epoch: 17.\tGenerator Avg.loss: 1.3031431436538696.\tDiscriminator Avg.loss: 0.9986674189567566.\n",
            "Epoch: 18.\tGenerator Avg.loss: 1.110051155090332.\tDiscriminator Avg.loss: 1.1005098819732666.\n",
            "Epoch: 19.\tGenerator Avg.loss: 1.3848038911819458.\tDiscriminator Avg.loss: 0.9480563998222351.\n",
            "Epoch: 20.\tGenerator Avg.loss: 1.384485125541687.\tDiscriminator Avg.loss: 0.9426612257957458.\n",
            "Epoch: 21.\tGenerator Avg.loss: 1.301444172859192.\tDiscriminator Avg.loss: 0.983845055103302.\n",
            "Epoch: 22.\tGenerator Avg.loss: 1.2194067239761353.\tDiscriminator Avg.loss: 1.0580265522003174.\n",
            "Epoch: 23.\tGenerator Avg.loss: 1.2963988780975342.\tDiscriminator Avg.loss: 0.9832645058631897.\n",
            "Epoch: 24.\tGenerator Avg.loss: 1.2811598777770996.\tDiscriminator Avg.loss: 0.9873733520507812.\n",
            "Epoch: 25.\tGenerator Avg.loss: 1.2221111059188843.\tDiscriminator Avg.loss: 1.0465757846832275.\n",
            "Epoch: 26.\tGenerator Avg.loss: 1.171006202697754.\tDiscriminator Avg.loss: 1.0666489601135254.\n",
            "Epoch: 27.\tGenerator Avg.loss: 1.1112505197525024.\tDiscriminator Avg.loss: 1.1008244752883911.\n",
            "Epoch: 28.\tGenerator Avg.loss: 1.2172126770019531.\tDiscriminator Avg.loss: 1.037514090538025.\n",
            "Epoch: 29.\tGenerator Avg.loss: 1.2128584384918213.\tDiscriminator Avg.loss: 1.0233099460601807.\n",
            "Epoch: 30.\tGenerator Avg.loss: 1.2033615112304688.\tDiscriminator Avg.loss: 1.0523496866226196.\n",
            "Epoch: 31.\tGenerator Avg.loss: 1.3542773723602295.\tDiscriminator Avg.loss: 0.9522630572319031.\n",
            "Epoch: 32.\tGenerator Avg.loss: 1.1203662157058716.\tDiscriminator Avg.loss: 1.1064690351486206.\n",
            "Epoch: 33.\tGenerator Avg.loss: 1.2358191013336182.\tDiscriminator Avg.loss: 1.0355268716812134.\n",
            "Epoch: 34.\tGenerator Avg.loss: 1.3589468002319336.\tDiscriminator Avg.loss: 0.9545416831970215.\n",
            "Epoch: 35.\tGenerator Avg.loss: 1.3709098100662231.\tDiscriminator Avg.loss: 0.9581896066665649.\n",
            "Epoch: 36.\tGenerator Avg.loss: 1.3196754455566406.\tDiscriminator Avg.loss: 0.9969671368598938.\n",
            "Epoch: 37.\tGenerator Avg.loss: 1.3914881944656372.\tDiscriminator Avg.loss: 0.9245954155921936.\n",
            "Epoch: 38.\tGenerator Avg.loss: 1.4384651184082031.\tDiscriminator Avg.loss: 0.9396122097969055.\n",
            "Epoch: 39.\tGenerator Avg.loss: 1.2230072021484375.\tDiscriminator Avg.loss: 1.0316386222839355.\n",
            "Epoch: 40.\tGenerator Avg.loss: 1.2964518070220947.\tDiscriminator Avg.loss: 0.9804745316505432.\n",
            "Epoch: 41.\tGenerator Avg.loss: 1.3649964332580566.\tDiscriminator Avg.loss: 0.9655482769012451.\n",
            "Epoch: 42.\tGenerator Avg.loss: 1.543837070465088.\tDiscriminator Avg.loss: 0.834227442741394.\n",
            "Epoch: 43.\tGenerator Avg.loss: 1.5000413656234741.\tDiscriminator Avg.loss: 0.8837684392929077.\n",
            "Epoch: 44.\tGenerator Avg.loss: 1.4442657232284546.\tDiscriminator Avg.loss: 0.9240943193435669.\n",
            "Epoch: 45.\tGenerator Avg.loss: 1.4954147338867188.\tDiscriminator Avg.loss: 0.8984029293060303.\n",
            "Epoch: 46.\tGenerator Avg.loss: 1.500749945640564.\tDiscriminator Avg.loss: 0.8848488330841064.\n",
            "Epoch: 47.\tGenerator Avg.loss: 1.6521903276443481.\tDiscriminator Avg.loss: 0.7986671924591064.\n",
            "Epoch: 48.\tGenerator Avg.loss: 1.607276201248169.\tDiscriminator Avg.loss: 0.8479861617088318.\n",
            "Epoch: 49.\tGenerator Avg.loss: 1.4176597595214844.\tDiscriminator Avg.loss: 0.9266020059585571.\n",
            "Epoch: 50.\tGenerator Avg.loss: 1.3867710828781128.\tDiscriminator Avg.loss: 0.9238351583480835.\n",
            "Epoch: 51.\tGenerator Avg.loss: 1.5428906679153442.\tDiscriminator Avg.loss: 0.8631958365440369.\n",
            "Epoch: 52.\tGenerator Avg.loss: 1.6122984886169434.\tDiscriminator Avg.loss: 0.8225411176681519.\n",
            "Epoch: 53.\tGenerator Avg.loss: 1.5783898830413818.\tDiscriminator Avg.loss: 0.8442080616950989.\n",
            "Epoch: 54.\tGenerator Avg.loss: 1.4727810621261597.\tDiscriminator Avg.loss: 0.8945953249931335.\n",
            "Epoch: 55.\tGenerator Avg.loss: 1.5057371854782104.\tDiscriminator Avg.loss: 0.880947470664978.\n",
            "Epoch: 56.\tGenerator Avg.loss: 1.4842816591262817.\tDiscriminator Avg.loss: 0.8708364963531494.\n",
            "Epoch: 57.\tGenerator Avg.loss: 1.5484638214111328.\tDiscriminator Avg.loss: 0.8461633920669556.\n",
            "Epoch: 58.\tGenerator Avg.loss: 1.410891056060791.\tDiscriminator Avg.loss: 0.9185541868209839.\n",
            "Epoch: 59.\tGenerator Avg.loss: 1.498843789100647.\tDiscriminator Avg.loss: 0.8652241230010986.\n",
            "Epoch: 60.\tGenerator Avg.loss: 1.3585503101348877.\tDiscriminator Avg.loss: 0.9433044195175171.\n",
            "Epoch: 61.\tGenerator Avg.loss: 1.4309718608856201.\tDiscriminator Avg.loss: 0.897111713886261.\n",
            "Epoch: 62.\tGenerator Avg.loss: 1.3285917043685913.\tDiscriminator Avg.loss: 0.9789295196533203.\n",
            "Epoch: 63.\tGenerator Avg.loss: 1.396291732788086.\tDiscriminator Avg.loss: 0.9201843738555908.\n",
            "Epoch: 64.\tGenerator Avg.loss: 1.3267730474472046.\tDiscriminator Avg.loss: 0.9781310558319092.\n",
            "Epoch: 65.\tGenerator Avg.loss: 1.3294954299926758.\tDiscriminator Avg.loss: 0.963764488697052.\n",
            "Epoch: 66.\tGenerator Avg.loss: 1.5031611919403076.\tDiscriminator Avg.loss: 0.8604862093925476.\n",
            "Epoch: 67.\tGenerator Avg.loss: 1.3353393077850342.\tDiscriminator Avg.loss: 0.9783875346183777.\n",
            "Epoch: 68.\tGenerator Avg.loss: 1.533074140548706.\tDiscriminator Avg.loss: 0.8563133478164673.\n",
            "Epoch: 69.\tGenerator Avg.loss: 1.4473118782043457.\tDiscriminator Avg.loss: 0.9390477538108826.\n",
            "Epoch: 70.\tGenerator Avg.loss: 1.500685691833496.\tDiscriminator Avg.loss: 0.8891130089759827.\n",
            "Epoch: 71.\tGenerator Avg.loss: 1.401218295097351.\tDiscriminator Avg.loss: 0.9561320543289185.\n",
            "Epoch: 72.\tGenerator Avg.loss: 1.3547065258026123.\tDiscriminator Avg.loss: 0.9701377749443054.\n",
            "Epoch: 73.\tGenerator Avg.loss: 1.4725230932235718.\tDiscriminator Avg.loss: 0.8822253346443176.\n",
            "Epoch: 74.\tGenerator Avg.loss: 1.3576875925064087.\tDiscriminator Avg.loss: 0.9571840763092041.\n",
            "Epoch: 75.\tGenerator Avg.loss: 1.3236761093139648.\tDiscriminator Avg.loss: 0.9490999579429626.\n",
            "Epoch: 76.\tGenerator Avg.loss: 1.3547592163085938.\tDiscriminator Avg.loss: 0.9450916647911072.\n",
            "Epoch: 77.\tGenerator Avg.loss: 1.3586426973342896.\tDiscriminator Avg.loss: 0.9454998970031738.\n",
            "Epoch: 78.\tGenerator Avg.loss: 1.2143681049346924.\tDiscriminator Avg.loss: 1.056587815284729.\n",
            "Epoch: 79.\tGenerator Avg.loss: 1.1784225702285767.\tDiscriminator Avg.loss: 1.0405628681182861.\n",
            "Epoch: 80.\tGenerator Avg.loss: 1.25795316696167.\tDiscriminator Avg.loss: 0.989149808883667.\n",
            "Epoch: 81.\tGenerator Avg.loss: 1.1105897426605225.\tDiscriminator Avg.loss: 1.1058106422424316.\n",
            "Epoch: 82.\tGenerator Avg.loss: 1.2055201530456543.\tDiscriminator Avg.loss: 1.013260006904602.\n",
            "Epoch: 83.\tGenerator Avg.loss: 1.0982890129089355.\tDiscriminator Avg.loss: 1.117686152458191.\n",
            "Epoch: 84.\tGenerator Avg.loss: 1.1045200824737549.\tDiscriminator Avg.loss: 1.1324048042297363.\n",
            "Epoch: 85.\tGenerator Avg.loss: 1.1059750318527222.\tDiscriminator Avg.loss: 1.1048085689544678.\n",
            "Epoch: 86.\tGenerator Avg.loss: 1.0606801509857178.\tDiscriminator Avg.loss: 1.147940993309021.\n",
            "Epoch: 87.\tGenerator Avg.loss: 0.9938238263130188.\tDiscriminator Avg.loss: 1.1893306970596313.\n",
            "Epoch: 88.\tGenerator Avg.loss: 1.0070420503616333.\tDiscriminator Avg.loss: 1.1687514781951904.\n",
            "Epoch: 89.\tGenerator Avg.loss: 0.9121308922767639.\tDiscriminator Avg.loss: 1.2471330165863037.\n",
            "Epoch: 90.\tGenerator Avg.loss: 0.8167858719825745.\tDiscriminator Avg.loss: 1.3321219682693481.\n",
            "Epoch: 91.\tGenerator Avg.loss: 0.8042534589767456.\tDiscriminator Avg.loss: 1.31346595287323.\n",
            "Epoch: 92.\tGenerator Avg.loss: 0.81830233335495.\tDiscriminator Avg.loss: 1.32179856300354.\n",
            "Epoch: 93.\tGenerator Avg.loss: 18.279403686523438.\tDiscriminator Avg.loss: 0.6814844608306885.\n",
            "Epoch: 94.\tGenerator Avg.loss: 46.626747131347656.\tDiscriminator Avg.loss: 0.02725580707192421.\n",
            "Epoch: 95.\tGenerator Avg.loss: 47.79063034057617.\tDiscriminator Avg.loss: 0.0032270930241793394.\n",
            "Epoch: 96.\tGenerator Avg.loss: 33.92164993286133.\tDiscriminator Avg.loss: 0.0644831508398056.\n",
            "Epoch: 97.\tGenerator Avg.loss: 30.25669288635254.\tDiscriminator Avg.loss: 0.0023859606590121984.\n",
            "Epoch: 98.\tGenerator Avg.loss: 42.93451690673828.\tDiscriminator Avg.loss: 0.0026645308826118708.\n",
            "Epoch: 99.\tGenerator Avg.loss: 28.045940399169922.\tDiscriminator Avg.loss: 0.0013603601837530732.\n",
            "Epoch: 100.\tGenerator Avg.loss: 24.244319915771484.\tDiscriminator Avg.loss: 0.04935233294963837.\n",
            "Epoch: 101.\tGenerator Avg.loss: 26.098329544067383.\tDiscriminator Avg.loss: 0.0009185548988170922.\n",
            "Epoch: 102.\tGenerator Avg.loss: 28.070032119750977.\tDiscriminator Avg.loss: 0.028657151386141777.\n",
            "Epoch: 103.\tGenerator Avg.loss: 28.21526527404785.\tDiscriminator Avg.loss: 0.007161698304116726.\n",
            "Epoch: 104.\tGenerator Avg.loss: 30.329587936401367.\tDiscriminator Avg.loss: 0.0012684877729043365.\n",
            "Epoch: 105.\tGenerator Avg.loss: 25.110811233520508.\tDiscriminator Avg.loss: 0.046431466937065125.\n",
            "Epoch: 106.\tGenerator Avg.loss: 30.030929565429688.\tDiscriminator Avg.loss: 0.04666650667786598.\n",
            "Epoch: 107.\tGenerator Avg.loss: 28.812210083007812.\tDiscriminator Avg.loss: 0.009895460680127144.\n",
            "Epoch: 108.\tGenerator Avg.loss: 30.90170669555664.\tDiscriminator Avg.loss: 0.026571907103061676.\n",
            "Epoch: 109.\tGenerator Avg.loss: 23.140832901000977.\tDiscriminator Avg.loss: 0.04290804639458656.\n",
            "Epoch: 110.\tGenerator Avg.loss: 28.469324111938477.\tDiscriminator Avg.loss: 0.001278206706047058.\n",
            "Epoch: 111.\tGenerator Avg.loss: 28.847841262817383.\tDiscriminator Avg.loss: 0.020464686676859856.\n",
            "Epoch: 112.\tGenerator Avg.loss: 56.55820846557617.\tDiscriminator Avg.loss: 0.00621722312644124.\n",
            "Epoch: 113.\tGenerator Avg.loss: 25.708192825317383.\tDiscriminator Avg.loss: 0.061281051486730576.\n",
            "Epoch: 114.\tGenerator Avg.loss: 26.40485954284668.\tDiscriminator Avg.loss: 0.02031813934445381.\n",
            "Epoch: 115.\tGenerator Avg.loss: 30.17517852783203.\tDiscriminator Avg.loss: 0.01043875515460968.\n",
            "Epoch: 116.\tGenerator Avg.loss: 20.000736236572266.\tDiscriminator Avg.loss: 0.07278845459222794.\n",
            "Epoch: 117.\tGenerator Avg.loss: 19.729969024658203.\tDiscriminator Avg.loss: 0.06195858493447304.\n",
            "Epoch: 118.\tGenerator Avg.loss: 15.352893829345703.\tDiscriminator Avg.loss: 0.11128386855125427.\n",
            "Epoch: 119.\tGenerator Avg.loss: 14.271883010864258.\tDiscriminator Avg.loss: 0.13279546797275543.\n",
            "Epoch: 120.\tGenerator Avg.loss: 12.435184478759766.\tDiscriminator Avg.loss: 0.12716259062290192.\n",
            "Epoch: 121.\tGenerator Avg.loss: 11.486595153808594.\tDiscriminator Avg.loss: 0.14840351045131683.\n",
            "Epoch: 122.\tGenerator Avg.loss: 10.227056503295898.\tDiscriminator Avg.loss: 0.17654059827327728.\n",
            "Epoch: 123.\tGenerator Avg.loss: 8.681082725524902.\tDiscriminator Avg.loss: 0.19887913763523102.\n",
            "Epoch: 124.\tGenerator Avg.loss: 7.58278751373291.\tDiscriminator Avg.loss: 0.2365424931049347.\n",
            "Epoch: 125.\tGenerator Avg.loss: 6.0179314613342285.\tDiscriminator Avg.loss: 0.31052303314208984.\n",
            "Epoch: 126.\tGenerator Avg.loss: 5.283014297485352.\tDiscriminator Avg.loss: 0.33224841952323914.\n",
            "Epoch: 127.\tGenerator Avg.loss: 5.0293965339660645.\tDiscriminator Avg.loss: 0.36667126417160034.\n",
            "Epoch: 128.\tGenerator Avg.loss: 4.979576110839844.\tDiscriminator Avg.loss: 0.3787548243999481.\n",
            "Epoch: 129.\tGenerator Avg.loss: 4.785309791564941.\tDiscriminator Avg.loss: 0.376838356256485.\n",
            "Epoch: 130.\tGenerator Avg.loss: 4.469155311584473.\tDiscriminator Avg.loss: 0.4045358896255493.\n",
            "Epoch: 131.\tGenerator Avg.loss: 4.177608489990234.\tDiscriminator Avg.loss: 0.4329676628112793.\n",
            "Epoch: 132.\tGenerator Avg.loss: 4.034653186798096.\tDiscriminator Avg.loss: 0.47204452753067017.\n",
            "Epoch: 133.\tGenerator Avg.loss: 3.768620014190674.\tDiscriminator Avg.loss: 0.5098534226417542.\n",
            "Epoch: 134.\tGenerator Avg.loss: 3.2383038997650146.\tDiscriminator Avg.loss: 0.5990332365036011.\n",
            "Epoch: 135.\tGenerator Avg.loss: 1.5373647212982178.\tDiscriminator Avg.loss: 1.0498020648956299.\n",
            "Epoch: 136.\tGenerator Avg.loss: 0.8235487341880798.\tDiscriminator Avg.loss: 1.283345341682434.\n",
            "Epoch: 137.\tGenerator Avg.loss: 0.8921560049057007.\tDiscriminator Avg.loss: 1.2633017301559448.\n",
            "Epoch: 138.\tGenerator Avg.loss: 0.9225672483444214.\tDiscriminator Avg.loss: 1.2231394052505493.\n",
            "Epoch: 139.\tGenerator Avg.loss: 0.9926284551620483.\tDiscriminator Avg.loss: 1.1831073760986328.\n",
            "Epoch: 140.\tGenerator Avg.loss: 1.00912344455719.\tDiscriminator Avg.loss: 1.1747206449508667.\n",
            "Epoch: 141.\tGenerator Avg.loss: 1.1985139846801758.\tDiscriminator Avg.loss: 1.114256739616394.\n",
            "Epoch: 142.\tGenerator Avg.loss: 1.338808536529541.\tDiscriminator Avg.loss: 1.0258795022964478.\n",
            "Epoch: 143.\tGenerator Avg.loss: 1.270754337310791.\tDiscriminator Avg.loss: 1.0520565509796143.\n",
            "Epoch: 144.\tGenerator Avg.loss: 1.3665090799331665.\tDiscriminator Avg.loss: 1.0011645555496216.\n",
            "Epoch: 145.\tGenerator Avg.loss: 1.5641334056854248.\tDiscriminator Avg.loss: 0.9368783235549927.\n",
            "Epoch: 146.\tGenerator Avg.loss: 1.5448133945465088.\tDiscriminator Avg.loss: 0.9719407558441162.\n",
            "Epoch: 147.\tGenerator Avg.loss: 1.4748133420944214.\tDiscriminator Avg.loss: 0.9874351024627686.\n",
            "Epoch: 148.\tGenerator Avg.loss: 1.7241933345794678.\tDiscriminator Avg.loss: 0.8656792640686035.\n",
            "Epoch: 149.\tGenerator Avg.loss: 1.7173306941986084.\tDiscriminator Avg.loss: 0.8527168035507202.\n",
            "Epoch: 150.\tGenerator Avg.loss: 1.5822774171829224.\tDiscriminator Avg.loss: 0.9166830778121948.\n",
            "Epoch: 151.\tGenerator Avg.loss: 1.392378330230713.\tDiscriminator Avg.loss: 0.9859930276870728.\n",
            "Epoch: 152.\tGenerator Avg.loss: 1.537426233291626.\tDiscriminator Avg.loss: 0.9313180446624756.\n",
            "Epoch: 153.\tGenerator Avg.loss: 1.5757123231887817.\tDiscriminator Avg.loss: 0.9476611614227295.\n",
            "Epoch: 154.\tGenerator Avg.loss: 1.7345722913742065.\tDiscriminator Avg.loss: 0.8416702151298523.\n",
            "Epoch: 155.\tGenerator Avg.loss: 1.6918178796768188.\tDiscriminator Avg.loss: 0.8838910460472107.\n",
            "Epoch: 156.\tGenerator Avg.loss: 1.5904183387756348.\tDiscriminator Avg.loss: 0.9147719144821167.\n",
            "Epoch: 157.\tGenerator Avg.loss: 1.781079649925232.\tDiscriminator Avg.loss: 0.8459222316741943.\n",
            "Epoch: 158.\tGenerator Avg.loss: 1.845625400543213.\tDiscriminator Avg.loss: 0.8334332704544067.\n",
            "Epoch: 159.\tGenerator Avg.loss: 1.72761869430542.\tDiscriminator Avg.loss: 0.8588868975639343.\n",
            "Epoch: 160.\tGenerator Avg.loss: 1.9530303478240967.\tDiscriminator Avg.loss: 0.7798041701316833.\n",
            "Epoch: 161.\tGenerator Avg.loss: 2.035966634750366.\tDiscriminator Avg.loss: 0.7747837901115417.\n",
            "Epoch: 162.\tGenerator Avg.loss: 1.9291744232177734.\tDiscriminator Avg.loss: 0.7880730032920837.\n",
            "Epoch: 163.\tGenerator Avg.loss: 1.9598373174667358.\tDiscriminator Avg.loss: 0.7853636145591736.\n",
            "Epoch: 164.\tGenerator Avg.loss: 2.0634069442749023.\tDiscriminator Avg.loss: 0.738888144493103.\n",
            "Epoch: 165.\tGenerator Avg.loss: 1.8475439548492432.\tDiscriminator Avg.loss: 0.8071204423904419.\n",
            "Epoch: 166.\tGenerator Avg.loss: 1.955222487449646.\tDiscriminator Avg.loss: 0.7654933929443359.\n",
            "Epoch: 167.\tGenerator Avg.loss: 2.019291877746582.\tDiscriminator Avg.loss: 0.7667803168296814.\n",
            "Epoch: 168.\tGenerator Avg.loss: 1.883157730102539.\tDiscriminator Avg.loss: 0.8086822628974915.\n",
            "Epoch: 169.\tGenerator Avg.loss: 1.921800971031189.\tDiscriminator Avg.loss: 0.7813352942466736.\n",
            "Epoch: 170.\tGenerator Avg.loss: 1.8266921043395996.\tDiscriminator Avg.loss: 0.8123100399971008.\n",
            "Epoch: 171.\tGenerator Avg.loss: 1.6753007173538208.\tDiscriminator Avg.loss: 0.8812664747238159.\n",
            "Epoch: 172.\tGenerator Avg.loss: 1.841313123703003.\tDiscriminator Avg.loss: 0.8140609264373779.\n",
            "Epoch: 173.\tGenerator Avg.loss: 1.7792620658874512.\tDiscriminator Avg.loss: 0.8242722153663635.\n",
            "Epoch: 174.\tGenerator Avg.loss: 1.65477454662323.\tDiscriminator Avg.loss: 0.8767829537391663.\n",
            "Epoch: 175.\tGenerator Avg.loss: 1.6587046384811401.\tDiscriminator Avg.loss: 0.8799057006835938.\n",
            "Epoch: 176.\tGenerator Avg.loss: 1.8817371129989624.\tDiscriminator Avg.loss: 0.8268055319786072.\n",
            "Epoch: 177.\tGenerator Avg.loss: 1.795508623123169.\tDiscriminator Avg.loss: 0.8468988537788391.\n",
            "Epoch: 178.\tGenerator Avg.loss: 1.837681770324707.\tDiscriminator Avg.loss: 0.840544581413269.\n",
            "Epoch: 179.\tGenerator Avg.loss: 1.9530302286148071.\tDiscriminator Avg.loss: 0.7962177991867065.\n",
            "Epoch: 180.\tGenerator Avg.loss: 1.9122990369796753.\tDiscriminator Avg.loss: 0.7922463417053223.\n",
            "Epoch: 181.\tGenerator Avg.loss: 1.8626044988632202.\tDiscriminator Avg.loss: 0.812671959400177.\n",
            "Epoch: 182.\tGenerator Avg.loss: 1.8736159801483154.\tDiscriminator Avg.loss: 0.8249899744987488.\n",
            "Epoch: 183.\tGenerator Avg.loss: 1.783551573753357.\tDiscriminator Avg.loss: 0.8252335786819458.\n",
            "Epoch: 184.\tGenerator Avg.loss: 1.779863715171814.\tDiscriminator Avg.loss: 0.8384813666343689.\n",
            "Epoch: 185.\tGenerator Avg.loss: 1.6323622465133667.\tDiscriminator Avg.loss: 0.8846027255058289.\n",
            "Epoch: 186.\tGenerator Avg.loss: 1.5992720127105713.\tDiscriminator Avg.loss: 0.9096912145614624.\n",
            "Epoch: 187.\tGenerator Avg.loss: 1.6496809720993042.\tDiscriminator Avg.loss: 0.8948163390159607.\n",
            "Epoch: 188.\tGenerator Avg.loss: 1.8149257898330688.\tDiscriminator Avg.loss: 0.8375241756439209.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-199-f457dafe1e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using CPU.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-198-5889ef3bb907>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_dataloader, config, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgenerator_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mdiscriminator_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdiscriminator_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mdiscriminator_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mgenerator_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenerator_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-196-05ea4e3fcbdf>\u001b[0m in \u001b[0;36mdiscriminator_train\u001b[0;34m(config, data, discriminator_optimizer, discriminator, generator, loss_function)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# weight updation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdiscriminator_total_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}